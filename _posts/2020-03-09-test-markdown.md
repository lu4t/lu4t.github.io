---
layout: post
title: La seguridad Multi Cloud.
subtitle: La transición.
gh-repo: lu4t/lu4t.github.io
gh-badge: [star, fork, follow]
tags: [Cloud][Security]
comments: true
---

Cuando hablamos de seguridad en múltiples nubes, lo más útil es comenzar hablando de ​​cómo era la seguridad en un centro de datos (privado) tradicional, o “Legacy”.

Tradicionalmente, lo habitual en seguridad era un enfoque centrado en el perímetro. Simplificando mucho: todo nuestro DataCenter estaba rodeado por cuatro paredes, y todo el tráfico estaba restringido a pasar por un único “canuto”. En ese “canuto” se desplegaban un montón de appliances, middleware y software para asegurarlo; se ponían cosas como: firewalls, balanceadores, WAFs, IDS,… toda una serie de soluciones de seguridad que eran diferentes, y en general de distintos fabricantes, para controlar el tráfico que pasaba por el punto único de entrada y salida.

El objetivo que realmente estábamos tratando de conseguir era establecer una diferencia entre el exterior de nuestro DC, que considerábamos una zona de baja confianza, y el interior, que considerábamos una zona de confianza elevada; de una manera tácita, estábamos reconociendo que cualquier tráfico que pudiese pasar por todos los dispositivos y appliances que hacían de punto de control era conforme con nuestras políticas, y por tanto podíamos confiar en él; si el tráfico que pasaba por el punto de control cumplía con nuestras políticas, el riesgo era asumible, y dejábamos pasar ese tráfico al interior del DC.

Hacíamos esta clara distinción, decíamos que todo lo que está sucediendo dentro del DC cuenta con los privilegios necesarios, y podíamos confiar en que el riesgo estaba controlado. Lo más típico en una organización grande era tomar medidas encaminadas a reducir la superficie de exposición al riesgo, y se acometieron proyectos y medidas orientadas a hacer una buena segmentación de redes; en las organizaciones más avanzadas se implementaron las VLANs, o incluso se mecanismo de virtualización de redes con SDN (redes definidas por software). Así que se comenzó a dividir la red de nuestro DC en varias partes, y cada uno de esos trozos de red podía contener cientos de servicios; en ocasiones nos encontrábamos que esos servicios se corresponden a, por ejemplo, una determinada línea de negocio…

El problema viene cuando nos decidimos a evolucionar a una arquitectura que incluye múltiples Clouds. Porque comenzamos a hacer la transición de un mundo Legacy como el que acabamos de describir, y queremos llevarnos determinada carga de trabajo desde éste a diferentes Clouds. Ejemplos podrían ser: mover el sistema de correo electrónico tradicional, el sistema de nóminas, y una aplicación de negocio refactorizada a micro-servicios, y las vamos a mover a un proveedor Cloud; supongamos que parte de esas cargas decidimos que acaben en una región de AWS, otras en una región Azure, la analítica de una aplicación que sigue on-prem la quiero ejecutar en Google, las cargas vmWare en IBMCloud… con este planteamiento, vamos a seguir necesitando que parte de nuestro DC siga conectado con esas nubes, y para eso utilizaremos VPNs, o conexiones punto a punto directas (si por el motivo que sea no quiero que ese tipo de tráfico vaya por internet).

En cuanto demos ese primer paso, varias dudas van a surgir de inmediato. La primera duda razonable es cómo de cierta sigue siendo la hipótesis de que tenemos un perímetro definido que proteger. Porque estos entornos no tienen cuatro paredes como tenía nuestro DC. Son entornos extraordinariamente seguros, probablemente más seguros que nuestro DC nunca podría llegar a ser con nuestro presupuesto y conocimiento técnico; pero no es menos cierto que de buenas a primeras estamos “a un API de distancia” de que cualquiera de los nodos de estos entornos puedan recibir o enviar tráfico a/desde Internet.

Nuestra hipótesis basada en la existencia de un perímetro de red que era fijo y conocido, se pone en cuestión por culpa de estos entornos Cloud.

Además, siendo honestos, nunca hubo un solo punto de entrada y salida a nuestro DC. Siempre han existido puntos “oscuros” que permitían conexiones por VPN a nuestra red corporativa, que aunque estaban también de alguna manera protegidos, permitían el tráfico sin pasar por ese punto central que describimos al principio.

Consecuentemente la idea de que tenemos un perímetro que proteger se hace difícil de mantener, y en su lugar tenemos que empezar a pensar en cómo debería de ser nuestra arquitectura de seguridad, sin un perímetro como primera línea de defensa.

El otro cambio de mentalidad necesario consiste en aceptar que la mayoría de nuestras amenazas y brechas, van a suceder desde el interior del “no-perímetro”. Hay dos ejemplos muy ilustrativos e interesantes que nos ayudan a ilustrar estos dos conceptos. El primero en concreto, para hablar de la ausencia de un perímetro, es el que se publicó contra los supermercados “Target”; en él, los datos personales y financieros de sus clientes (de las tarjetas de fidelización y tarjetas de crédito asociadas) fueron robados a través del sistema de control del Aire Acondicionado. ¿Cómo puede ser posible que desde el sistema de control del aire acondicionado de un supermercado, se pueda llegar al sistema central alojado en el DC corporativo que custodia la información de las tarjetas de fidelización de clientes y sus tarjetas de crédito?. Y el hecho es que, por grande que sean unos grandes almacenes, su arquitectura de red y sistemas se parece mucho al entorno que acabamos de describir (de manera muy simplificada, eso si); siempre hay un servicio cloud, que tiene conectividad por VPN a nuestro centro de datos, y por tanto siempre hay algunas puertas que llegarán nuestro DC. La forma en que los atacantes entraron fue a través de una contraseña Wi-Fi débil que les permitió entrar en la red de una de las tiendas, y de ahí se conectaron a la red del DC. Como vemos se aplicó la hipótesis de que si estas dentro del perímetro, estás en una red que es de confianza, por lo que pudo acceder a esta base de datos, tomar los datos que querían y marcharse con ellos. Este es un ataque que demuestra, lo primero: la dependemos que tenemos de cómo de bien defendido está el perímetro, y lo segundo: que siempre acaba siendo permeable.

El otro ejemplo es el de los papeles de Panamá; hablamos de que unos subcontratados, que tenían credenciales de VPN, tenía la capacidad de iniciar sesión y acceder a sistemas que requieren ciertos privilegios. Este ataque, igual que muchos otros, es desde el interior de la organización, no fue hecho desde el exterior.

Del análisis detallado de esta serie de acontecimientos, surge lo que comúnmente llamamos hoy el Zero Trust Networking, que no es más que una especie de filosofía o creencia que se resume en que: en lugar de creer que estar en nuestra red interna tiene asociado algún nivel de acceso implícito, o cierto nivel de confianza, lo que vamos a creer es que este hecho no implica nada en términos de acceso a los sistemas. Dicho de otro modo: por el mero hecho de estar conectado a mi red, no vas a tener más acceso que alguien que está fuera de la ella. Así que cuando comenzamos a adoptar este enfoque, que insisto, es más que nada un tipo de filosofía de seguridad, hay muchas cosas que comienzan a cambiar.

Hay algunas cosas que se convierten en realmente críticas. La primera que se vuelve esencial es la gestión de credenciales y certificados (“secrets”). Aquí hablamos de administrar todas las credenciales que nos dan acceso a sistemas que requieren de algún privilegio.

Un ejemplo de esto: tenemos un servidor web en el DC, y mi servidor web tiene la capacidad de hablar con una base de datos, que está en otro segmento de red del DC; Esta BBDD tiene un usuario y contraseña que definen el acceso a ese sistema. Tradicionalmente lo que veríamos es que estas credenciales de acceso no se manejan con mucho cuidado en el ámbito de los DBAs y Desarrolladores; hay ocasiones en que estaban “hardcoded” en texto plano dentro del código la aplicación, o metidas en los ficheros de configuración, o en texto plano en nuestro sistema de control de versiones,… y la razón por la que se hacía esto, es porque siempre pensamos que estaba en un lugar de confianza y conocido, la base de datos estaba dentro del perímetro de alta confianza, y dentro de este seguramente en una subred o VLAN a la que no tenía acceso nadie que no debiese. Asumiamos el riesgo, y las credenciales eran fácilmente accesibles, porque nadie sin autorización podría acceder a nuestra base de datos si no estaba en nuestra red. Y el desafío es que tal vez alguien que ya está en nuestra red porque puede, haga un uso no adecuado de la contraseña de nuestra base de datos. Así es que, lo que está claro es que deberíamos proteger estos secretos con mucho más cuidado. Tenemos que encriptarlos, y que desplegar controles de acceso a ellos, tenemos que auditar quién tiene acceso a las credenciales, controlar que solo se dan en caso de una necesidad justificada,…

La otra cosa que se vuelve esencial si te haces seguidor de la filosofía Zero Trust, es la necesidad de segmentar el tráfico, o mejor aún como veremos: segmentar los servicios. Porque cuando hablamos de segmentación, siempre pensamos en esta imagen del DC legacy tradicional, en la de teníamos segmentos y subredes relativamente grandes, en las que estamos usábamos VLANs o SDNs para segmentar; y todas estas zonas de red tenían cientos de servicios que corrían en sistemas; pero ahora, al mover carga a AWS o Azure, nuestras capacidades de segmentación son diferentes: para segmentar en AWS tenemos security groups, en Azure tenemos virtual networks… ¿Conclusión?, que nuestras VLAN no son interoperables con nuestra virtual network, y nuestra virtual network no es interoperable con nuestro security group, y nuestro security group tampoco sabe nada acerca de nuestras VLANs; así es que: ¿cómo segmentamos nuestra red entre estos entornos tan dispares?… porque ahora voy a tener una aplicación en Azure, que está llamando a una base de datos en mi DC,… ¿cómo voy a forzar que se cumpla mi política de acceso de quién tiene permitido hablar con quién?.

En arquitecturas Cloud modernas, lo ideal es sustituir la segmentación de redes tradicional por una segmentación de servicios. Y así, definir nuestras políticas a un nivel más lógico y menos físico.

Tengo que poder decir que nuestro servidor web tiene permitido hablar con nuestra base de datos. O que nuestro Gateway de APIs tiene permitido hablar con nuestro servidor web.

Con este enfoque, dejamos de pensar en APIs concretas que hablan entre ellas como hablan los servidores; no decimos que la API #1 puede hablar con la API #2, si no que debemos hablar de ellas en términos de identidades.

Si seguimos esta pauta, diremos que “el servicio identificado como servidor web puede hablar con el servicio identificado como base de datos”; y en este contexto, estar identificado querrá decir que está debidamente autorizado, ha sido autenticado, y podremos contabilizar (hacer accounting) del acceso.

Si operamos de esta manera, son muy relevantes las ventajas que obtendremos. En primer lugar, la política de seguridad se cumplirá en un contexto de carga elástica (en la que infraestructura se expande o contrae en función de la carga), no importa si tenemos diez o cien instancias de servidor web en un momento determinado, la política es la misma y se controla de un modo en que el volumen de carga no tiene ninguna relevancia. En una arquitectura tradicional, este tipo de políticas se traducen en reglas de firewall, que se convierten a su vez en conjunto de direcciones IP, la consecuencia de ello es que la reglas tienen un crecimiento exponencial ligado al tamaño de la infraestructura.

La segunda ventaja es que, si no pensamos en direcciones IP, simplificamos mucho cómo implementamos la segmentación. No importa si hay comunicaciones a través de VPNs, o si una aplicación está corriendo en varias direcciones IP, o si las direcciones se están solapando, o si varían con el tiempo. Si trabajamos con un enfoque tradicional pensando en IPs, cualquier traducción o cambio de dirección IP, tiene un impacto negativo en los controles de seguridad, o en el servicio.

En arquitecturas de microservicios, debemos de pensar en términos de segmentación de servicios, y no de segmentación a de nivel de red.

Pongamos otro ejemplo que lo demuestra, si hemos implementado una segmentación de red tradicional, no va a haber manera de extenderla a las redes de AWS, Azure, Google, o IBMCloud, Nunca van a entender la noción de VLAN que hemo implementado en nuestra red, por lo que necesitamos trabajar en un nivel de abstracción superior, y trabajar con ese mismo concepto de segmentación (filosofía ZeroTrust), pero a nivel servicios.

El siguiente reto a considerar es si la política de protección de los datos que habíamos diseñado en nuestro DC, es igual de efectiva en esta nueva arquitectura.

Volviendo al ejemplo de antes, es muy probable que nuestro servidor web esté escribiendo sus datos en nuestra base de datos sin cifrar; y esto es nuevamente el resultado de la suposición de que estamos dentro de nuestro perímetro. Asumimos de nuevo que todos los que trabajan para nosotros, y que tienen acceso a nuestra red, son de confianza. Por lo que el riesgo de que los datos estén sin cifrar en la base de datos era asumible, y en cualquier caso menor que el impacto que sufriríamos si lo hiciésemos. A lo sumo, empleamos algún tipo de control de acceso a las credenciales necesarias para poder leerlos.

Pero una vez que cambiamos los supuestos de partida, si asumimos que no queremos confíar en el operador de mi base de datos porque que puede ser una amenaza interna (en potencia), o asumimos como probable que un atacante pueda llegar a mi red interna, el riesgo se incrementa y pasa a ser muy importante proteger la integridad de nuestros datos. Entonces, lo que se vuelve esencial son los datos que están en reposo, ya sea en un almacén de objetos, en una base de datos, o en los sistemas de ficheros de red; todos esos datos deben estar encriptados si quieres que estén protegidos. La ventaja inicial es que si alguno de los riesgos anteriores se materializa, ahora necesitarán que también rompan mi sistema de cifrado. Para acceder a mi base de datos y poder leer los datos, también debe poder romper mi algoritmo de cifrado, u obtener acceso a mis claves de cifrado para poder descifrarlos.

En resumen, una buena gestión de secretos, una segmentación de servicios que escale y una política de protección del dato adecuada, son las claves a considerar; se trata de cómo añadir barreras que hagan que sea más difícil exfiltrar nuestros datos y que salgan fuera de nuestro dominio sin control. Los dos primeros son posiblemente los más simples de implementar, porque no necesariamente tenemos que modificar nuestras aplicaciones; las aplicaciones existentes seguramente se podrán cambiar de una manera rápida con simplemente modificar parte de su configuración, o la parte de código que afecta a los secretos. Evolucionar a una segmentación por servicios también puede ser realizada de forma transparente a las aplicaciones. Quizá el último paso a abordar sea el relacionado con la protección del dato. Si hablamos en términos de curva de madurez, los dos primeros pasos son lineales en términos de dificultad, mientras que una buena protección de datos requiere más integración a nivel de aplicación.

Todos estos conceptos que hemos comentado hasta aquí, no son nuevos. Pero entonces, ¿por qué decimos que es necesaria una nueva forma de pensar, y que es diferente del enfoque tradicional?. El motivo es que las herramientas consideradas tradicionales parten de una concepción de infraestructura estática; fueron diseñadas para funcionar en un mundo donde la rotación de nuestra infraestructura no era muy elevada, porque trabajabamos en un DC privado. Adicionalmente, la mayoría de las herramientas que usamos fueron diseñadas para un mundo basado en direcciones IP; tanto si hablamos de la gestión de accesos privilegiados tradicional, como si hablamos de firewalls, su unidad de administración primordial es una dirección IP; esta unidad es difícil de gestionar si nos planteamos un mundo que escala dinámicamente, o que varía mucho y a gran velocidad, o que se sustenta en el principio de nubes híbridas multiproveedor. Por contra, las necesidades actuales necesitan dar solución a la transición hacia entornos híbridos, las herramientas más modernas para securizar estas arquitecturas de microservicios se centran en un entorno que, por diseño, es más dinámico; y a medida que evolucionamos hacia un DataCenter dinámico, esperamos que la infraestructura aparezca y desaparezca en función de la carga gestionada en cada momento; el uso cada vez más habitual de contenedores, hace que necesitamos un enfoque diferente; que gestionemos la seguridad de un entorno como este basándonos en direcciones IP, se vuelve demasiado complejo; en su lugar, un enfoque orientado a servicios donde la unidad de gestión es la identidad, si asignamos identidades a los servicios, simplificará enormemente esta transición.

Presentado el enfoque general y las motivaciones principales, en los post que publicaré en el futuro profundizaremos en los aspectos más relevantes de la gestión de secretos y el service mesh; con ellos veremos cómo conseguir facilitar la transición a una nube híbrida, sin dejar de lado nuestras políticas de seguridad.